% !TEX root =  main.tex
\chapter{Configuration} \label{ch:configuration}

\section{Main configuration file}
The main configuration file is \texttt{vpp/vpp\_server/resources/config.ini}.
In the repository, a \texttt{config.ini.default} is provided. It is shown and explained below:

\lstset{language=C, basicstyle=\ttfamily}  
\begin{lstlisting}
[DB]
user=[FILL IN]
password=[FILL IN]
host=[external address of localhost] 
database=vpp
measurement_partition_period_hours=24 
rolling_window_length_days=7

[DB-DW]
enabled=False
user=[FILL IN]
password=[FILL IN]
host=[host.sub.domain.dk]
database=vpp_dw

[LOG]
level=DEBUG
\end{lstlisting}

\subsection{Section \texttt{[DB]}}
Section \texttt{[DB]} specifies how to connect to the Rolling Window DB that is intended to run on the same machine as the VPP application. Note that it is perfectly possible to run the database on a different machine if desired.

\paragraph{\texttt{host}} This should be the external address (hostname or IP) of the machine running the database - usually the same as is running the application. While writing \texttt{localhost} is sufficient for the application to access the DB, this address is also used by the data warehouse to extract data for export, and therefore it must be the machine's actual network address.
Port \texttt{5432} (the PostgreSQL default) is assumed. Note that this port must be accessible from the data warehouse machine, as the data export SQL is sent to and executed in the data warehouse DB, connecting back to the main DB.

\paragraph{\texttt{database}} The name of the database to connect to. 

\paragraph{\texttt{measurement\_partition\_period\_hours}} Incoming measurements and predictions will be stored in subtables covering time intervals of the number of hours specified here. Should be a divisor of 24 (e.g 24, 12, 6). Behavior with other values has not been tested. The partitioning happens with respect to UTC, not the local time of the server.

\paragraph{\texttt{rolling\_window\_length\_days}} The number of days to retain subtables with measurements and predictions before exporting the data to data warehouse and deleting the tables. The current day is not counted, meaning that a value of 1 will cause data received during June 10th to be exported sometime during June 12th, since the last data of June 10th will at that time be at least 24 hours old. Behavior with value 0 has not been tested.

\subsection{Section \texttt{[DB-DW]}}

\paragraph{\texttt{enabled}} If \texttt{true}, data export to data warehouse and subsequent local deletion will be performed. If \texttt{false}, data will be retained in the local database forever. In this case, configuration of user, password, host and database name may be omitted, and the {\texttt{rolling\_window\_length\_days} value above is irrelevant. Values are not case sensitive.
	
\subsection{Section \texttt{[LOG]}}
\paragraph{\texttt{level}} Specifies the level of logging that will be output in \\ \texttt{vpp\_server/logs/console.log}. Valid values (case sensitive): \texttt{DEBUG, INFO, WARNING, ERROR, FATAL, CRITICAL} (the last two are equal) and integer values. \texttt{DEBUG} is level 10. Setting value 9 will add messages on files skipped when fetching from FTP servers. The log level can be changed while the application is running and should take effect within 5 seconds.

No file rotation is performed, which can cause log file to grow very large if the log level is set to \texttt{INFO} or lower and a high volume of messages is received. It is recommended to set level \texttt{WARNING}, in which case any problems will be reported, but nothing else.

Note that the log file is cleared on application startup.

\section{Data provider: measurements from FTP}

The retrieval of data is configured by placing a \texttt{.ini} file for each data source in folder \texttt{vpp\_server/resources/data\_providers}. 


File \texttt{ftp\_energinet\_online.ini}:
\begin{lstlisting}
[data_provider]
adapter = vpp.data_acquisition.adapter.ftp_adapter.FTPAdapter
interpreter = vpp.data_acquisition.interpreter.energinet_online_interpreter.EnerginetOnlineInterpreter
processor = vpp.data_acquisition.processing_strategy.DefaultMeasurementProcessingStrategy
id_prefix = energinet

[fetch]
interval = 600
adapter_date_strategy = vpp.data_acquisition.adapter.adapter_date_strategy.DefaultAdapterFileDateStrategy
last_fetch_adapter = 2016-05-27T00:00:00
fetch_again_when_date_equal = True
last_fetch_interpreter = 2016-05-27T14:55:00

[ftp]
host = 194.239.2.256
username = ftp000123
password = ...
port = 21
remote_dir = Onlinedata
file_pattern = ([0-9]{4})([0-1][0-9])([0-3][0-9])_onlinedata\.txt
encoding = iso-8859-1

[averaging]
enable = True
id_patterns = energinet_1; energinet_2; energinet_3; energinet_%%
intervals = 900; 1800; 3600; 7200
\end{lstlisting}

\subsection{Section \texttt{[data\_provider]}}
This section configures the protocol for obtaining data, and how they should be interpreted and stored. The adapters, interpreters and processors (listed below) can be combined in any way, but the interpreter obviously must be matched to the data fetched/received by the data provider. It also would be pointless to combine a PredictionProcessingStrategy with a data source and interpreter that provides measurements.

\paragraph{\texttt{adapter}} This specifies which data adapter to use for retrieving data from the provider. Available adapters:

\texttt{vpp.data\_acquisition.adapter.ftp\_adapter.FTPAdapter}

\texttt{[...].ftp\_adapter.SFTPAdapter}

\texttt{[...].rabbit\_mq\_adapter.RabbitMQAdapter}

The adapter will handle communication with the provider and extract a raw text body and pass it to the data interpreter.

\paragraph{\texttt{interpreter}} This specifies which data interpreter to use for parsing the received raw text messages and transforming them to an internal data format that is then passed to the processor responsible for storing measurements (or predictions) in the database. Available interpreters:

\texttt{vpp.data\_acquisition.interpreter.energinet\_online\_interpreter.EnerginetOnlineInterpreter}

\texttt{[...].energinet\_co2\_interpreter.EnerginetCO2Interpreter}

\texttt{[...].nordpoolspot\_interpreter.NordpoolspotInterpreter}

\texttt{[...].thor\_interpreter.ThorInterpreter}

\texttt{[...].nrgi\_abs\_interpreter.NrgiAbsInterpreter}

\texttt{[...].nrgi\_delta\_interpreter.NrgiDeltaInterpreter}

\texttt{[...].grundfos\_data\_interpreter.GrundfosDataInterpreter}

\texttt{[...].smartamm\_data\_interpreter.SmartAmmDataInterpreter}

\paragraph{\texttt{processor}} Responsible for storing measurements or predictions in the database. Available processors:

\texttt{vpp.data\_acquisition.processing\_strategy.DefaultMeasurementProcessingStrategy}

\texttt{vpp.data\_acquisition.processing\_strategy.DefaultPredictionProcessingStrategy}

\paragraph{\texttt{id\_prefix}} Among the data received will be sensors (logical sources of measurements) and endpoints (logical sources of predictions). The id\_prefix specifies a prefix for the database ID of the sensors and endpoints which makes it easier to distinguish them and their measurements/predictions in the database.


\subsection{Section \texttt{[fetch]}}
Since the FTPAdapter must actively fetch data, this section is required. 

\paragraph{\texttt{interval}} In seconds. Every time this interval passes, the FTPAdapter will connect  and fetch data which is then processed and stored. The time to next fetch is measured from launch, not finish, meaning that the interval will be maintained precisely as long as the fetch and processing completes before the next launch.

\paragraph{\texttt{adapter\_date\_strategy}} 
Strategy for deciding whether a file should be fetched, based on the date extracted from the file name. Available strategies:

\texttt{vpp.data\_acquisition.adapter.adapter\_date\_strategy.DefaultAdapterFileDateStrategy}

\texttt{vpp.data\_acquisition.adapter.adapter\_date\_strategy.CO2FileDateStrategy}


The \texttt{DefaultAdapterFileDateStrategy} will fetch a file if the date is later (or equal to, depending on the value of \texttt{fetch\_again\_when\_date\_equal}) than the value of \texttt{last\_fetch\_adapter}.

\texttt{CO2FileDateStrategy} will fetch if the date extracted from the filename is in the future compared to the time of execution. This is useful for the CO2 predictions from Energinet.


\paragraph{\texttt{fetch\_again\_when\_date\_equal}}  Specifies whether files with timestamp identical to the value of \texttt{last\_fetch\_adapter} should be fetched. Setting this to \texttt{True} will enable repeated retrieval of the same file(s), which can be useful if file contents are updated.

\paragraph{\texttt{last\_fetch\_adapter}} Updated by the data adapter when fetching files to the latest timestamp extracted from the fetched filenames. If configuring a new data provider, set this to a value close to the current time to avoid fetching many old files.

\paragraph{\texttt{last\_fetch\_interpreter}} Updated by the data interpreter to the timestamp of the latest processed measurement/prediction. This is relevant in the case the same file is continuously updated with new data.


\subsection{Section \texttt{[ftp]}}

\texttt{host, username, password} and \texttt{port} should be self-explanatory.

\paragraph{\texttt{remote\_dir}} is the subdirectory to access on the FTP server. Use forward slashes.

\paragraph{\texttt{file\_pattern}} This should be a Python regex containing up to four groups which will match year, month, day and hour (in that order). Only year is required - the others are optional. Only files matching this pattern will be considered for fetching. The extracted timestamp is compared against the value of \texttt{last\_fetch\_adapter} to determine if the file should be fetched.

\paragraph{\texttt{encoding}} Specifies the encoding of the files to be fetched. Tested values are \texttt{utf-8, ascii} and \texttt{iso-8859-1}.


\subsection{Section \texttt{[averaging]}}

\paragraph{\texttt{enable}} If \texttt{True} or \texttt{true}, data transferred to data warehouse will be averaged over the intervals specified below in order to reduce data size in data warehouse. If not, entries will be bulk copied as-is.

\paragraph{\texttt{id\_patterns}} List of patterns of sensor/endpoint IDs to perform averaging for. Patterns are used in a PostgreSQL \texttt{LIKE} comparison. The wildcard \texttt{\%} must be written twice due to Python's string parsing. Thus, to match all sensors starting with prefix \texttt{energinet}, write \texttt{energinet\%\%}. Patterns should be separated with semicolon \texttt{;}. Every pattern should have a corresponding interval in the \texttt{intervals} list below. The patterns are processed in order, so it makes sense to add a catch-all pattern at the end of the list.

\paragraph{\texttt{intervals}}
Semicolon-separated list of time intervals in seconds, each interval corresponding to the pattern with the same position in the list above.


\section{Data provider: measurements from RabbitMQ}
File \texttt{....ini}:
\begin{lstlisting}
[data_provider]
adapter = 
vpp.data_acquisition.adapter.rabbit_mq_adapter.RabbitMQAdapter
interpreter = 
vpp.data_acquisition.interpreter.smartamm_data_interpreter.SmartAmmDataInterpreter
processor = 
vpp.data_acquisition.processing_strategy.DefaultMeasurementProcessingStrategy
id_prefix = smartamm

[exchange]
username = user123
password = 12345
serverAddressList = ecosensemq02.cs.au.dk
port = 5672
sslConnection = False
exchangeName = ecosense-exchange
exchangeDurable = True
exchangeType = topic

[queue]
name = vpp-listener-queue-new
routingKeys = smartamm.#
durable = False
exclusive = False
autoDelete = True

[ssl_options]
cacerts = /etc/rabbitmq/ssl/tesca/cacert.pem
certfile = /etc/rabbitmq/ssl/client/cert.pem
keyfile = /etc/rabbitmq/ssl/client/key.pem
cert_reqs = verify_peer
fail_if_no_peer_cert = True 

[averaging]
enable = False
id_patterns = smartamm%%RMSCurrent%%; smartamm%%ActivePower%%;
intervals = 60; 120
\end{lstlisting}

\subsection{Section \texttt{[exchange]}}
Since the FTPAdapter must actively fetch data, this section is required. 

\subsection{Section \texttt{[queue]}}

\subsection{Section \texttt{[ssl\_options]}}